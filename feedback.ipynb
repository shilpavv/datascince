{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feedback.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6i2iMK7FrpmASIZwdiSS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shilpavv/datascince/blob/master/feedback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRrGgq17b6VU",
        "outputId": "df8edc28-9ce1-421b-f162-e61c690c1374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "accuracy score: 0.2\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris=load_iris()\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(iris.data,iris.target,test_size=0.2,random_state=1)\n",
        "\n",
        "#hidden_layer_sizes: tuple, length = n_layers - 2, default=(100,)\n",
        "#The ith element represents the number of neurons in the ith hidden layer. (6,) means one hidden layer with 6 neurons\n",
        "#solver:\n",
        "#The weight optimization can be influenced with the solver parameter. Three solver modes are available\n",
        "#    'lbfgs'\n",
        "#    is an optimizer in the family of quasi-Newton methods.\n",
        "#   'sgd'\n",
        "#   refers to stochastic gradient descent.\n",
        "#   'adam' refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n",
        "#Without understanding in the details of the solvers, you should know the following: 'adam' works pretty well - both training time and validation score - on relatively large datasets, i.e. thousands of training samples or more. For small datasets, however, 'lbfgs' can converge faster and perform better.\n",
        "#'alpha'\n",
        "#This parameter can be used to control possible 'overfitting' and 'underfitting'. \n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(solver='lbfgs', \n",
        "\n",
        "                    alpha=1e-5,\n",
        "\n",
        "                    hidden_layer_sizes=(6,), \n",
        "\n",
        "                    random_state=1)\n",
        "\n",
        "clf.fit(x_train,y_train)     \n",
        "clf.score(x_train,y_train) \n",
        "y_pred=clf.predict(x_test)\n",
        "print(y_pred)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"accuracy score:\",accuracy_score(y_test,y_pred))"
      ]
    }
  ]
}